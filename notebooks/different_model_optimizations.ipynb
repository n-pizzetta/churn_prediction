{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rruVqSpmasbX"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.utils.validation import check_array\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#-------- Defining the custom transformers --------#\n",
        "\n",
        "class DropNaN(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns_list: Optional[List[str]] = None, reset_index: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Transformer that drops rows containing NaN values in specified columns or all columns.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        columns_list : list of str, optional (default=None)\n",
        "            List of column names to check for NaN values. If None, all columns are checked.\n",
        "        reset_index : bool, optional (default=False)\n",
        "            If True, resets the index of the returned DataFrame after dropping rows.\n",
        "        \"\"\"\n",
        "        self.columns_list = columns_list\n",
        "        self.reset_index = reset_index\n",
        "\n",
        "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y=None) -> 'DropNaN':\n",
        "        # Validate input\n",
        "        X = self._validate_input(X)\n",
        "\n",
        "        # Store column names\n",
        "        self.feature_names_in_ = X.columns.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: Union[pd.DataFrame, np.ndarray], y=None) -> Union[pd.DataFrame, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Removes rows containing missing values in the specified columns or in all columns if none are specified.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, DataFrame}, shape (n_samples, n_features)\n",
        "            The data to clean.\n",
        "\n",
        "        y : array-like, shape (n_samples,), optional (default=None)\n",
        "            Target values (ignored in this transformer).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X_transformed : same as input type\n",
        "            `X` with rows containing missing values removed.\n",
        "\n",
        "        y_transformed : same as y\n",
        "            `y` with corresponding rows removed if `y` is provided.\n",
        "        \"\"\"\n",
        "        is_array = isinstance(X, np.ndarray)\n",
        "        X = self._validate_input(X)\n",
        "\n",
        "        # Check for consistency in columns\n",
        "        if X.columns.tolist() != self.feature_names_in_:\n",
        "            raise ValueError(\"The columns in the input data during transform differ from those during fit.\")\n",
        "\n",
        "        # Determine columns to check for NaN\n",
        "        cols_to_check = self.columns_list if self.columns_list else X.columns\n",
        "\n",
        "        # Check if specified columns exist\n",
        "        missing_cols = [col for col in cols_to_check if col not in X.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"The following columns were not found: {missing_cols}\")\n",
        "\n",
        "        # Drop rows with NaN values in specified columns\n",
        "        if y is not None:\n",
        "            y = pd.Series(y, name='target') if not isinstance(y, pd.Series) else y\n",
        "            Xy = pd.concat([X, y], axis=1)\n",
        "            Xy_transformed = Xy.dropna(subset=cols_to_check)\n",
        "            X_transformed = Xy_transformed.drop(columns=[y.name])\n",
        "            y_transformed = Xy_transformed[y.name]\n",
        "\n",
        "            # Reset index if required\n",
        "            if self.reset_index:\n",
        "                X_transformed.reset_index(drop=True, inplace=True)\n",
        "                y_transformed.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            # Convert back to original type if input was an array\n",
        "            if is_array:\n",
        "                X_transformed = X_transformed.values\n",
        "                y_transformed = y_transformed.values\n",
        "\n",
        "            return X_transformed, y_transformed\n",
        "        else:\n",
        "            X_transformed = X.dropna(subset=cols_to_check)\n",
        "\n",
        "            # Reset index if required\n",
        "            if self.reset_index:\n",
        "                X_transformed.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            # Convert back to original type if input was an array\n",
        "            if is_array:\n",
        "                X_transformed = X_transformed.values\n",
        "\n",
        "            return X_transformed\n",
        "\n",
        "    def _validate_input(self, X):\n",
        "        # Validate X and convert to DataFrame if necessary\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return X.copy()\n",
        "        elif isinstance(X, np.ndarray):\n",
        "            X = check_array(X, ensure_2d=True, allow_nd=False, dtype=None)\n",
        "            return pd.DataFrame(X, columns=getattr(self, 'feature_names_in_', None))\n",
        "        else:\n",
        "            raise TypeError(\"Input must be a pandas DataFrame or a NumPy array.\")\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None) -> List[str]:\n",
        "        # Return the feature names\n",
        "        return self.feature_names_in_ if input_features is None else input_features\n",
        "\n",
        "\n",
        "class SeniorStatusTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column='customer_senior'):\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # Nothing to fit\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = X[self.column].map({0: 'No', 1: 'Yes'}).astype('object')\n",
        "        return X\n",
        "\n",
        "\n",
        "class PipelineWithY(Pipeline):\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        for name, transform in self.steps:\n",
        "            if hasattr(transform, 'fit_transform'):\n",
        "                # Attempt to fit_transform with both X and y\n",
        "                try:\n",
        "                    result = transform.fit_transform(X, y)\n",
        "                    if isinstance(result, tuple) and len(result) == 2:\n",
        "                        X, y = result\n",
        "                    else:\n",
        "                        X = result\n",
        "                except TypeError:\n",
        "                    # Transformer doesn't accept y\n",
        "                    X = transform.fit_transform(X)\n",
        "            else:\n",
        "                X = transform.fit(X, y).transform(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        for name, transform in self.steps:\n",
        "            if hasattr(transform, 'transform'):\n",
        "                try:\n",
        "                    result = transform.transform(X, y)\n",
        "                    if isinstance(result, tuple) and len(result) == 2:\n",
        "                        X, y = result\n",
        "                    else:\n",
        "                        X = result\n",
        "                except TypeError:\n",
        "                    # Transformer doesn't accept y\n",
        "                    X = transform.transform(X)\n",
        "        if y is not None:\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "\n",
        "def create_full_pipeline(df):\n",
        "    #-------- Defining input variables --------#\n",
        "    id_col = \"id\"\n",
        "    target_col = \"churn\"\n",
        "    senior_col = \"customer_senior\"\n",
        "\n",
        "    drop_columns = [\"phone_subscription\", \"streaming_tv\"]\n",
        "    excluded_columns = drop_columns + [id_col, target_col, senior_col]\n",
        "\n",
        "    # Identifying categorical and numerical columns to process\n",
        "    cat_columns = [senior_col] + [col for col in df.select_dtypes(include=['object']).columns if col not in excluded_columns]\n",
        "    num_columns = [col for col in df.select_dtypes(include=[np.number]).columns if col not in excluded_columns]\n",
        "\n",
        "    #-------- Defining the transformers --------#\n",
        "    cat_transformer = Pipeline(\n",
        "        steps=[\n",
        "            ('encoder', OneHotEncoder(sparse_output=False))\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    num_transformer = Pipeline(\n",
        "        steps=[\n",
        "            ('scaler', StandardScaler())\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    #-------- Defining the preparation pipeline --------#\n",
        "    preparation = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('col_drop', 'drop', drop_columns),\n",
        "            ('cat', cat_transformer, cat_columns),\n",
        "            ('num', num_transformer, num_columns)\n",
        "        ],\n",
        "        remainder='passthrough',\n",
        "        verbose_feature_names_out=False\n",
        "    )\n",
        "\n",
        "    #-------- Defining the full pipeline --------#\n",
        "    full_pipeline = PipelineWithY(\n",
        "        steps=[\n",
        "            ('drop_nan', DropNaN(columns_list=None, reset_index=True)),\n",
        "            ('senior_status', SeniorStatusTransformer(column=senior_col)),\n",
        "            ('preparation', preparation)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return full_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the root directory to the path\n",
        "#sys.path.append(os.path.abspath(\"..\"))\n",
        "\n",
        "#from utils.pipeline import create_full_pipeline\n",
        "\n",
        "data = pd.read_csv(\"data.csv\", delimiter=\",\")\n",
        "full_pipeline = create_full_pipeline(data)\n",
        "\n",
        "X = data.drop(columns=[\"churn\"])\n",
        "y = data[\"churn\"]\n",
        "y = y.map({'No': 0, 'Yes': 1})\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Transform the training data\n",
        "X_train_transformed, y_train_transformed = full_pipeline.transform(X_train, y_train)\n",
        "\n",
        "# Transform the test data\n",
        "X_test_transformed, y_test_transformed = full_pipeline.transform(X_test, y_test)\n",
        "\n",
        "X_train_transformed = pd.DataFrame(X_train_transformed, columns=full_pipeline.named_steps['preparation'].get_feature_names_out())\n",
        "#y_train_transformed = pd.DataFrame(y_train_transformed)\n",
        "\n",
        "X_test_transformed = pd.DataFrame(X_test_transformed, columns=full_pipeline.named_steps['preparation'].get_feature_names_out())\n",
        "#y_test_transformed = pd.DataFrame(y_test_transformed)\n",
        "\n",
        "# Keep the ids\n",
        "id_train = X_train_transformed[\"id\"]\n",
        "X_train_transformed.drop(columns=\"id\", inplace=True)\n",
        "\n",
        "id_test = X_test_transformed[\"id\"]\n",
        "X_test_transformed.drop(columns=\"id\", inplace=True)"
      ],
      "metadata": {
        "id": "rwKxENwua0yv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering on the full transformed dataset (combine train and test)\n",
        "X_transformed = pd.concat([X_train_transformed, X_test_transformed], ignore_index=True)\n",
        "id_column = pd.concat([id_train, id_test], ignore_index=True)"
      ],
      "metadata": {
        "id": "oN4wmQVGa5xL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split back into train and test sets after clustering\n",
        "X_train_transformed = X_transformed.iloc[:len(X_train_transformed), :]\n",
        "X_test_transformed = X_transformed.iloc[len(X_train_transformed):, :]\n",
        "# Convert all object columns in X_train_transformed and X_test_transformed to numeric\n",
        "X_train_transformed = X_train_transformed.apply(pd.to_numeric, errors='coerce')\n",
        "X_test_transformed = X_test_transformed.apply(pd.to_numeric, errors='coerce')"
      ],
      "metadata": {
        "id": "s9fx0z9qbAHw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XG Boost with oversampling on the minority class ad Randomized Search"
      ],
      "metadata": {
        "id": "baycwZSSbS7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Oversample the minority class with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X_train_transformed, y_train_transformed)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0],\n",
        "    'scale_pos_weight': [1, 2, 3],\n",
        "    'gamma': [0, 0.1, 0.5, 1],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [1, 2, 5]\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42, eval_metric=\"logloss\")\n",
        "\n",
        "# Use RandomizedSearchCV with SMOTE-balanced data\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,  # Adjust based on computational resources\n",
        "    scoring='roc_auc',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_res, y_res)\n",
        "\n",
        "# Best parameters and performance\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best AUC-ROC Score:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = random_search.best_estimator_.predict(X_test_transformed)\n",
        "print(\"Test AUC-ROC:\", roc_auc_score(y_test_transformed, random_search.best_estimator_.predict_proba(X_test_transformed)[:, 1]))\n",
        "print(classification_report(y_test_transformed, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RfOmihobX2a",
        "outputId": "d4d6c700-0c78-47b3-8608-00d54b94a601"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best Parameters: {'subsample': 1.0, 'scale_pos_weight': 1, 'reg_lambda': 5, 'reg_alpha': 0, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 0.8}\n",
            "Best AUC-ROC Score: 0.9325652437465546\n",
            "Test AUC-ROC: 0.8538990558340133\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86      1035\n",
            "           1       0.62      0.63      0.63       373\n",
            "\n",
            "    accuracy                           0.80      1408\n",
            "   macro avg       0.74      0.75      0.75      1408\n",
            "weighted avg       0.80      0.80      0.80      1408\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define models and parameter grids for grid search\n",
        "models = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(solver='liblinear', random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 300, 500],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(probability=True, random_state=42),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf'],\n",
        "            'gamma': ['scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': xgb.XGBClassifier(objective='binary:logistic', random_state=42, eval_metric=\"logloss\"),\n",
        "        'params': {\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [4, 6, 8],\n",
        "            'n_estimators': [100, 300, 500],\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'colsample_bytree': [0.8, 1.0]\n",
        "        }\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model': lgb.LGBMClassifier(objective='binary', random_state=42),\n",
        "        'params': {\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [-1, 10, 20],\n",
        "            'n_estimators': [100, 300, 500],\n",
        "            'num_leaves': [31, 40, 50],\n",
        "            'subsample': [0.8, 1.0],\n",
        "            'colsample_bytree': [0.8, 1.0]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npwML7zqiC_y",
        "outputId": "7f739406-e24f-46a6-86b6-0bbed934ee08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different model"
      ],
      "metadata": {
        "id": "JZwTNttnxtf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for model_name, mp in models.items():\n",
        "    print(f\"Running Grid Search for {model_name}...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=mp['model'],\n",
        "        param_grid=mp['params'],\n",
        "        scoring='roc_auc',\n",
        "        cv=5,\n",
        "        verbose=1,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Fit the grid search to the data\n",
        "    grid_search.fit(X_train_transformed, y_train_transformed)\n",
        "\n",
        "    # Get the best estimator and performance\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test_transformed)\n",
        "    auc_score = roc_auc_score(y_test_transformed, best_model.predict_proba(X_test_transformed)[:, 1])\n",
        "\n",
        "    print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
        "    print(f\"{model_name} Test AUC-ROC: {auc_score}\")\n",
        "    print(f\"{model_name} Classification Report:\\n\", classification_report(y_test_transformed, y_pred))\n",
        "\n",
        "    # Save results\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Best Parameters': grid_search.best_params_,\n",
        "        'AUC-ROC': auc_score\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for easy viewing\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4B0aCKriXFH",
        "outputId": "1e4ecb6f-7268-4194-acd6-e075116d40b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Grid Search for Logistic Regression...\n",
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best Parameters for Logistic Regression: {'C': 10, 'penalty': 'l2'}\n",
            "Logistic Regression Test AUC-ROC: 0.8623421533201227\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88      1035\n",
            "           1       0.69      0.60      0.64       373\n",
            "\n",
            "    accuracy                           0.82      1408\n",
            "   macro avg       0.78      0.75      0.76      1408\n",
            "weighted avg       0.82      0.82      0.82      1408\n",
            "\n",
            "Running Grid Search for Random Forest...\n",
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Best Parameters for Random Forest: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 500}\n",
            "Random Forest Test AUC-ROC: 0.862865394827162\n",
            "Random Forest Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87      1035\n",
            "           1       0.68      0.52      0.59       373\n",
            "\n",
            "    accuracy                           0.81      1408\n",
            "   macro avg       0.76      0.72      0.73      1408\n",
            "weighted avg       0.80      0.81      0.80      1408\n",
            "\n",
            "Running Grid Search for SVM...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Parameters for SVM: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "SVM Test AUC-ROC: 0.8526155599590732\n",
            "SVM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88      1035\n",
            "           1       0.69      0.60      0.64       373\n",
            "\n",
            "    accuracy                           0.82      1408\n",
            "   macro avg       0.78      0.75      0.76      1408\n",
            "weighted avg       0.82      0.82      0.82      1408\n",
            "\n",
            "Running Grid Search for XGBoost...\n",
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Best Parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'subsample': 0.8}\n",
            "XGBoost Test AUC-ROC: 0.8633627332892981\n",
            "XGBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88      1035\n",
            "           1       0.69      0.51      0.59       373\n",
            "\n",
            "    accuracy                           0.81      1408\n",
            "   macro avg       0.76      0.71      0.73      1408\n",
            "weighted avg       0.80      0.81      0.80      1408\n",
            "\n",
            "Running Grid Search for LightGBM...\n",
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1496, number of negative: 4128\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000859 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 659\n",
            "[LightGBM] [Info] Number of data points in the train set: 5624, number of used features: 41\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.266003 -> initscore=-1.014998\n",
            "[LightGBM] [Info] Start training from score -1.014998\n",
            "Best Parameters for LightGBM: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 300, 'num_leaves': 31, 'subsample': 0.8}\n",
            "LightGBM Test AUC-ROC: 0.860832005802282\n",
            "LightGBM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87      1035\n",
            "           1       0.67      0.51      0.58       373\n",
            "\n",
            "    accuracy                           0.81      1408\n",
            "   macro avg       0.76      0.71      0.73      1408\n",
            "weighted avg       0.79      0.81      0.80      1408\n",
            "\n",
            "                 Model                                    Best Parameters  \\\n",
            "0  Logistic Regression                         {'C': 10, 'penalty': 'l2'}   \n",
            "1        Random Forest  {'max_depth': 10, 'min_samples_leaf': 4, 'min_...   \n",
            "2                  SVM   {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}   \n",
            "3              XGBoost  {'colsample_bytree': 0.8, 'learning_rate': 0.0...   \n",
            "4             LightGBM  {'colsample_bytree': 0.8, 'learning_rate': 0.0...   \n",
            "\n",
            "    AUC-ROC  \n",
            "0  0.862342  \n",
            "1  0.862865  \n",
            "2  0.852616  \n",
            "3  0.863363  \n",
            "4  0.860832  \n"
          ]
        }
      ]
    }
  ]
}